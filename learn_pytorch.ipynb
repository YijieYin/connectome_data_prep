{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "929fdcfe-0441-425e-a56e-61f8cce10dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "527f1f65-e257-47da-b4be-5af6e4332976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [7, 5, 3]])\n"
     ]
    }
   ],
   "source": [
    "t0 = torch.tensor(1000)\n",
    "t1 = torch.tensor([9,8,6,5]) \n",
    "t2 = torch.tensor([[1,2,3],[7,5,3]])\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7f4ec4f-ae77-4cca-8aaf-8df4741a77ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((2,3), dtype = torch.int16)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67678a36-31c8-4e0f-a91a-79872d8d3fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63a02473-56c6-472d-ab0c-52f0f0609525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of dimensions\n",
    "a.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12aad5fa-2e3c-4635-b881-de34f9d81dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1]], dtype=torch.int16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the dimension of the tensor \n",
    "a.view(1,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59af0e0-c5f2-4f09-8588-35b6571db201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2725ed14-e23b-4f77-9831-670b9e117bd5",
   "metadata": {},
   "source": [
    "# basic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a0c7cba-3263-4cfc-9522-9397703858d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 6.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([1.0, 2.0]) \n",
    "v = torch.tensor([3.0, 4.0]) \n",
    "\n",
    "u+v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec3e8388-932f-4e2f-9495-3f2c881fb05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9237, 0.9060, 0.9258, 0.6086],\n",
       "        [0.9082, 0.4293, 0.2659, 0.2653]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7b047b5-6582-4076-8ce6-f30f5cee02c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1233,  0.7881,  0.1299,  0.4529],\n",
       "        [ 0.2722,  0.4946, -0.9476,  0.4599]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2,4) *2 -1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e53470cd-1102-4e62-b195-9c4397dec6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1233, 0.7881, 0.1299, 0.4529],\n",
       "        [0.2722, 0.4946, 0.9476, 0.4599]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c28f7ff-5cf3-4e5a-aadb-2f0bda6e6980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., -0., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ceil(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f069ec5-c39f-4a08-8322-6ecbebeb7ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1233,  0.5000,  0.1299,  0.4529],\n",
       "        [ 0.2722,  0.4946, -0.5000,  0.4599]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.clamp(a, -0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9bf7d41-336c-4b3b-82cb-d7ce5bc5df71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2217)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f8bf34c-3226-4dbb-a67b-849a439bb945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7881)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3744bf-af50-4160-8c41-9f02fec7b753",
   "metadata": {},
   "source": [
    "## trigonometric functions and their inverses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17077467-209b-4527-aca8-7de86a3098f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc0363be-0c50-4431-a064-9619096848e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000e+00,  1.0000e+00, -8.7423e-08])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([0, np.pi/2, np.pi])\n",
    "torch.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9aa25df-6eec-4ea9-9bdb-47799acab417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.0000, -1.0000, -1.0000, -1.0000, -0.9999])\n"
     ]
    }
   ],
   "source": [
    "# an evenly spaced list of numbers between a range: \n",
    "pi = torch.linspace(-np.pi/2, np.pi/2, steps = 1000) \n",
    "sined = torch.sin(pi) \n",
    "print(sined[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b98aa5c-0301-4c90-a043-d34caf92197e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(sined, label = 'sined') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a10f9-3613-422a-a479-4ae2e8eb5910",
   "metadata": {},
   "source": [
    "# understand pytorch numpy bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63d38c30-7a4c-4804-a636-7bbf66a01358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 4])\n"
     ]
    }
   ],
   "source": [
    "u = torch.tensor([2,4]) \n",
    "v = torch.tensor([1,4]) \n",
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdc159f0-8dce-4354-9014-19aa7fe06a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2, 16])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = u*v\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58558018-d6e0-4e0c-b6ff-83a8d02a5e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17b38ef0-c15f-4ebe-9c47-15e0b07a0060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5029794-e5d0-4756-8632-c6f660c1f264",
   "metadata": {},
   "source": [
    "Broadcasting: i.e., if you have one array of size 10, the other of size 5, and you add them together, you'll get an array of size 10. the smaller array is used iteratively twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf514184-7eb1-4df9-b2c1-7bde439fa7af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5460, 0.7837],\n",
       "         [0.5861, 0.0664],\n",
       "         [0.0031, 0.9178]],\n",
       "\n",
       "        [[0.5460, 0.7837],\n",
       "         [0.5861, 0.0664],\n",
       "         [0.0031, 0.9178]],\n",
       "\n",
       "        [[0.5460, 0.7837],\n",
       "         [0.5861, 0.0664],\n",
       "         [0.0031, 0.9178]],\n",
       "\n",
       "        [[0.5460, 0.7837],\n",
       "         [0.5861, 0.0664],\n",
       "         [0.0031, 0.9178]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(4,3,2)\n",
    "b = a*torch.rand(3,2) \n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19f3fce5-9c2a-4b47-a9fc-8f0c16dd268f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1135, 0.1135],\n",
       "         [0.5059, 0.5059],\n",
       "         [0.2494, 0.2494]],\n",
       "\n",
       "        [[0.1135, 0.1135],\n",
       "         [0.5059, 0.5059],\n",
       "         [0.2494, 0.2494]],\n",
       "\n",
       "        [[0.1135, 0.1135],\n",
       "         [0.5059, 0.5059],\n",
       "         [0.2494, 0.2494]],\n",
       "\n",
       "        [[0.1135, 0.1135],\n",
       "         [0.5059, 0.5059],\n",
       "         [0.2494, 0.2494]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*torch.rand(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0093cc07-3dcc-48b0-8ab6-d9be56d12afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0801, 0.8445],\n",
       "         [0.0801, 0.8445],\n",
       "         [0.0801, 0.8445]],\n",
       "\n",
       "        [[0.0801, 0.8445],\n",
       "         [0.0801, 0.8445],\n",
       "         [0.0801, 0.8445]],\n",
       "\n",
       "        [[0.0801, 0.8445],\n",
       "         [0.0801, 0.8445],\n",
       "         [0.0801, 0.8445]],\n",
       "\n",
       "        [[0.0801, 0.8445],\n",
       "         [0.0801, 0.8445],\n",
       "         [0.0801, 0.8445]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*torch.rand(1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a34e740-1e69-41a5-b67b-ff7ebaf6b005",
   "metadata": {},
   "source": [
    "## switching between ndarrays and pytorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c704e0db-6b5f-4271-8244-d13cf4b86851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2a2ee7a-7994-410e-9927-f73f06b67099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "ar = np.ones((2,3))\n",
    "ten = torch.from_numpy(ar)\n",
    "print(ar) \n",
    "print(ten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb6cebca-b3b0-42d6-a964-60da422011f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0f96e7-2bf9-46c7-9ae9-066536c1027c",
   "metadata": {},
   "source": [
    "NOTE: they use the same underlying memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b32f7acb-2715-437b-a26c-dd2c840da33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.],\n",
      "        [ 1., 23.,  1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "ar[1,1] = 23\n",
    "print(ten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f7e9b1-97bd-4f9a-ac31-7301229787a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09004f50-b18e-46a6-85b0-86a55729e8ec",
   "metadata": {},
   "source": [
    "# autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc08567c-ea1d-48be-b6c7-b4f47243358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b5d11c0-cea9-4867-9add-ecd5e79d4d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.ones(2,2), requires_grad = True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6131fa77-7a5b-4a60-83ef-b1a338726169",
   "metadata": {},
   "source": [
    "This is created as a result of an operation, so it has a grad function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b96dcdee-77bc-413b-81a3-909cda718f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3.],\n",
       "        [3., 3.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x + 2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf3402e3-22c7-4940-a818-35a18eeffab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x11c8e0dc0>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "66d760f0-4ea7-4017-8a04-084d421b7069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y*y*3\n",
    "out = z.mean()\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1863e4b5-3484-4433-b4ed-b2fdfabf1718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the gradient \n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58edeb91-4408-4eeb-be5b-33846da34ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.5000, 4.5000],\n",
       "        [4.5000, 4.5000]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216a82ee-2dac-4db8-a969-b12b94120325",
   "metadata": {},
   "source": [
    "## \"advanced\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3f2520ee-9971-480d-a887-c1e981e611ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1341, -0.0999, -1.0093], requires_grad=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3) \n",
    "x = Variable(x, requires_grad = True) \n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3187795f-3d68-4aa7-a6db-21a086d2e6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  137.2855,  -102.2957, -1033.5099], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x*2 \n",
    "while y.data.norm() <1000: \n",
    "    y = y*2 \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1a6dcfd1-c241-47d5-bb91-e045f206c989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  137.2855,  -102.2957, -1033.5099])\n",
      "tensor(1047.5946)\n"
     ]
    }
   ],
   "source": [
    "print(y.data)\n",
    "print(y.data.norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "57590e51-7950-4900-87e2-0489f738925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = torch.FloatTensor([0.1, 1.0, 0.0001])\n",
    "y.backward(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fd4562be-c472-4632-9944-5fcfde599731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046ab797-3de5-4614-b1ad-7235e95ae4a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2ed28aef-a2cb-4b77-88e1-33c0e083ebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torchvision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b28a5575-2dfb-42a7-ad8b-2ccb0522537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl \n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7f00a8a9-9f58-45cd-9fd6-a96508a56245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /Users/yijieyin/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c6fb52aba74513bf85922388f751fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d30b019c-4048-4af2-a9c8-a8e6babf277a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all parameters \n",
    "for param in model.parameters(): \n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "13dc58b3-657f-4bd5-9d9d-3b35b343ef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(512, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "34ce0a81-13f2-4eae-9126-f294c3abbe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = optim.SGD(model.parameters(), lr = 1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f1cb59d0-b1ae-457f-8480-653fd0b39aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(optimiser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d18dae-1c4f-412b-9b33-9fbf248a7ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e8e340c-dc68-4d06-8b4b-d81ac07a8bcf",
   "metadata": {},
   "source": [
    "# torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dae1a0b-c7ef-4704-8b15-2eb1a1a543ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f84333e-87a3-4e85-8064-7150ded1771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "\n",
    "import torch \n",
    "import torchvision.transforms as T \n",
    "from torchvision.io import read_image \n",
    "\n",
    "plt.rcParams['savefig.bbox'] = 'tight' \n",
    "torch.manual_seed(1) \n",
    "\n",
    "def show(imgs): \n",
    "    fix, axs = plt.subplots(ncols = len(imgs), squeeze = False) \n",
    "    for i, img in enumerate(imgs): \n",
    "        img = T.ToPILImage()(img.to('cpu'))\n",
    "        axs[0,i].imshow(np.asarray(img)) \n",
    "        axs[0,i].set(xticklables-[], yticklabels = [], xticks = [], yticks = []) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ab05cec-bf91-4929-ae1e-7d722fecb3bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'images/pottery1.jpeg'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(Path('images') / 'pottery1.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8667bfd0-2dec-45a3-a154-8ea63a444534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69da8b7-fe04-4a80-b627-cb3bb8229b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e010885-9db4-4017-8951-c9efc59370d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9866a7f5-2db5-4b4a-a580-7eae76138b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the adjacency matrix\n",
    "adj_matrix = torch.tensor([[0, 1, 1], [1, 0, 1], [1, 1, 0]], dtype=torch.float)\n",
    "\n",
    "# define the input tensor and set requires_grad=True to enable gradient computation\n",
    "x = torch.tensor([1.0, 1.0, 1.0], requires_grad=True)\n",
    "\n",
    "# define the target neuron index\n",
    "target_neuron = 2\n",
    "\n",
    "# define the ReLU activation function\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "# compute the output of the network\n",
    "h1 = relu(torch.matmul(adj_matrix, x))\n",
    "output = relu(torch.matmul(adj_matrix, h1))[target_neuron]\n",
    "\n",
    "# define the loss function as the negative activation of the target neuron\n",
    "loss = -output\n",
    "\n",
    "# perform backpropagation to compute the gradient of the loss with respect to the input\n",
    "loss.backward()\n",
    "\n",
    "# update the input based on the gradient\n",
    "learning_rate = 0.1\n",
    "with torch.no_grad():\n",
    "    x -= learning_rate * x.grad\n",
    "\n",
    "# reset the gradient to zero for the next iteration\n",
    "x.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30754662-26fd-423a-9960-3cb83f5019c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "60f67cb9-6e4f-47f7-b937-e8889c8aa4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5919, 0.9295, 0.5861])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2d52ab92-79f2-41a9-9ae7-e42c6dbeeaa2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n4/qdb8tn357cz8f4_fbq2x8rv80000gn/T/ipykernel_11809/1067595098.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# define the loss function as the negative activation of the target neuron\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Update the input image using gradient ascent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.11/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.11/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# define the adjacency matrix\n",
    "adj_matrix = torch.tensor([[0, 1, 1], [1, 0, 1], [1, 1, 0]], dtype=torch.float)\n",
    "\n",
    "# Define the learning rate and number of iterations for gradient ascent\n",
    "learning_rate = 0.1\n",
    "num_iterations = 100\n",
    "\n",
    "# Create a random input image to modify\n",
    "x = torch.rand(3)\n",
    "\n",
    "# Define the optimizer as gradient ascent on the input image\n",
    "optimizer = torch.optim.Adam([x], lr=learning_rate)\n",
    "\n",
    "# define the target neuron index\n",
    "target_neuron = 2\n",
    "\n",
    "# define the ReLU activation function\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "# Run the optimization loop for the specified number of iterations\n",
    "for i in range(num_iterations):\n",
    "    # Compute the loss and gradients\n",
    "    \n",
    "    # compute the output of the network\n",
    "    h1 = relu(torch.matmul(adj_matrix, x))\n",
    "    output = relu(torch.matmul(adj_matrix, h1))[target_neuron]\n",
    "\n",
    "    # define the loss function as the negative activation of the target neuron\n",
    "    loss = -output\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the input image using gradient ascent\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Zero out the gradients for the next iteration\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Clip the input image values to the range [0, 1]\n",
    "    x.clamp_(0, 1)\n",
    "\n",
    "    # reset the gradient to zero for the next iteration\n",
    "    x.grad.zero_()\n",
    "    print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5b965e-4735-4eae-bd17-ac847df8335d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bb524b60-3eca-44a0-8441-aa349055aa7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /Users/yijieyin/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e372335b3842d681a25095e3bc5b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/528M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [256, 256, 3, 3], expected input[1, 3, 224, 224] to have 256 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n4/qdb8tn357cz8f4_fbq2x8rv80000gn/T/ipykernel_11809/1726958969.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Compute the loss and gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.11/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/n4/qdb8tn357cz8f4_fbq2x8rv80000gn/T/ipykernel_11809/1726958969.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_neuron\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.11/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.11/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.11/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 442\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [256, 256, 3, 3], expected input[1, 3, 224, 224] to have 256 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Load a pre-trained model\n",
    "model = models.vgg16(pretrained=True)\n",
    "\n",
    "# Define the layer and neuron to maximize activation\n",
    "target_layer = model.features[12]\n",
    "target_neuron = 0\n",
    "\n",
    "# Define the learning rate and number of iterations for gradient ascent\n",
    "learning_rate = 0.1\n",
    "num_iterations = 100\n",
    "\n",
    "# Create a random input image to modify\n",
    "input_image = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "# Define the loss function as the negative activation of the target neuron\n",
    "class ActivationLoss(nn.Module):\n",
    "    def __init__(self, target_layer, target_neuron):\n",
    "        super().__init__()\n",
    "        self.target_layer = target_layer\n",
    "        self.target_neuron = target_neuron\n",
    "    \n",
    "    def forward(self, x):\n",
    "        activation = self.target_layer(x)[0][self.target_neuron]\n",
    "        return -activation\n",
    "\n",
    "# Create an instance of the loss function\n",
    "loss_function = ActivationLoss(target_layer, target_neuron)\n",
    "\n",
    "# Define the optimizer as gradient ascent on the input image\n",
    "optimizer = torch.optim.Adam([input_image], lr=learning_rate)\n",
    "\n",
    "# Run the optimization loop for the specified number of iterations\n",
    "for i in range(num_iterations):\n",
    "    # Compute the loss and gradients\n",
    "    loss = loss_function(input_image)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the input image using gradient ascent\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Zero out the gradients for the next iteration\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Clip the input image values to the range [0, 1]\n",
    "    input_image.clamp_(0, 1)\n",
    "\n",
    "# Convert the optimized input image to a numpy array and visualize it\n",
    "optimized_image = np.transpose(input_image.detach().numpy()[0], (1, 2, 0))\n",
    "Image.fromarray((optimized_image * 255).astype(np.uint8)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0f3d015a-f118-49d8-b822-356be7eff1cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [512, 512, 3, 3], expected input[1, 3, 224, 224] to have 512 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n4/qdb8tn357cz8f4_fbq2x8rv80000gn/T/ipykernel_11809/3141948044.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Compute the loss and gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_image_preprocessed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_neuron\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.11/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.11/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.11/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 442\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [512, 512, 3, 3], expected input[1, 3, 224, 224] to have 512 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# Load the pretrained network\n",
    "model = models.vgg16(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Select the target layer and neuron\n",
    "target_layer = model.features[28]\n",
    "target_neuron = 10\n",
    "\n",
    "# Define the loss function\n",
    "loss_function = torch.nn.MSELoss()\n",
    "\n",
    "# Load and preprocess the input image\n",
    "image_path = '/Users/yijieyin/Downloads/things_db8b96ba-bc21-4ce2-a0ae-136727773c21/aardvark/aardvark_02s.jpg'\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "image = transform(Image.open(image_path)).unsqueeze(0)\n",
    "\n",
    "# Initialize the input image to be optimized\n",
    "input_image = torch.randn_like(image, requires_grad=True)\n",
    "\n",
    "# Set the optimizer\n",
    "optimizer = torch.optim.Adam([input_image], lr=0.1)\n",
    "\n",
    "# Run the optimization loop\n",
    "num_iterations = 1000\n",
    "for i in range(num_iterations):\n",
    "    # Preprocess the input image to match the expected size and shape of the pretrained network\n",
    "    input_image_preprocessed = torch.nn.functional.interpolate(input_image, size=224, mode='bilinear', align_corners=False)\n",
    "    \n",
    "    # Compute the loss and gradients\n",
    "    activation = target_layer(input_image_preprocessed)[0][target_neuron]\n",
    "    loss = loss_function(activation, torch.tensor(1.0))\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the input image\n",
    "    optimizer.step()\n",
    "    input_image.data.clamp_(0, 1)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "# Save the optimized image\n",
    "output_image = transforms.functional.to_pil_image(input_image[0])\n",
    "output_image.save('/Users/yijieyin/Downloads/out_image.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3b017a2e-bf91-4039-9129-8fa208d5aadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_image_preprocessed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7e94732c-55ec-42ca-8a16-4d8e18e2b694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b903dc-3e1b-462d-82b9-9172947fa1eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
