{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49ba300d-a24b-42cd-8ef6-2f309bf14851",
   "metadata": {
    "tags": []
   },
   "source": [
    "# prepare the connectome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d012e6a-06b7-4624-a75f-18aac255ab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83942009-8e35-49de-addb-760427ae8993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ef7eb9-ad88-49a8-b71a-b4e695afc4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "set_seeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582030af-af61-4b6b-bea8-e8e6081433c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = pd.read_csv('/Users/yijieyin/Downloads/larva/Supplementary-Data-S1/ad_connectivity_matrix.csv', index_col=0)\n",
    "ad.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc07995-a854-48a0-92b4-dfc4910ef957",
   "metadata": {},
   "source": [
    "So the columns and rows are skids, and the values are synapse numbers, not input proportion. So we need the total number of synapses on the dendrites for each skid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a81e052-6e9c-42e4-80c4-f79fac95c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum across rows, for each column \n",
    "# ad.sum(axis='rows')\n",
    "# calculate input proportion. Divide for each column \n",
    "ad_inprop = ad.div(ad.sum(axis = 'rows'),axis = 'columns')\n",
    "ad_inprop.fillna(0, inplace=True)\n",
    "# turn index into string from int64 \n",
    "ad_inprop.index = ad_inprop.index.map(str)\n",
    "ad_inprop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b12b5e1-c743-409a-8eb9-31fed9c2e591",
   "metadata": {},
   "source": [
    "## inhibitory neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15af634a-fc49-491a-a05f-733e4a51bfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymaid\n",
    "\n",
    "rm = pymaid.connect_catmaid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b516ee-ea0c-45ae-8505-79d3e3562c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaba = pymaid.find_neurons(annotations='GABA')\n",
    "glu = pymaid.find_neurons(annotations='glutamate')\n",
    "inhi = list(gaba.skeleton_id) + list(glu.skeleton_id)\n",
    "\n",
    "inhibitory_selected = [skid for skid in inhi if skid in ad_inprop.index]\n",
    "ad_inprop.loc[inhibitory_selected] = ad_inprop.loc[inhibitory_selected].apply(lambda x: x * -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68acf8d8-cd1f-4653-86c2-d44d6e95926a",
   "metadata": {},
   "source": [
    "## get meta info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8638c2fd-50b6-4fe3-85dc-712780f280e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv('/Users/yijieyin/Downloads/larva/brain-neurons_meta-data.csv')\n",
    "meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70883563-bcc3-4c61-b8d3-42958e61e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many cells on one side (roughly)? \n",
    "meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cf528a-6ad7-41c8-9fac-fb33299bbbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and which types are there? \n",
    "meta.celltype.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7f4047-3f1f-45d8-b182-4f1eaa1e5322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what about sub-type?\n",
    "meta.annotated_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc8a57e-fa23-46d6-add4-11caed27b71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â how many neurons on the right without a contralateral homologue? \n",
    "sum(meta.leftid == 'no pair')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a010c1ff-c384-4d3c-b8e9-7cd8465314b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(meta.rightid == 'no pair')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8703c06d-419d-4014-8973-bd17bb2dee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which kind of sensory neurons are there? \n",
    "meta[meta.celltype.isin(['sensory'])]['annotated_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfea15a-86dc-4e9e-b171-27f49a89d829",
   "metadata": {},
   "source": [
    "## make a type dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907ae4c3-d041-4f3d-a45a-69306328ab4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are many values in the left_id or right_id column that is 'no pair'. So multiple values are assigned to the 'no pair' key. \n",
    "# when this happens, only the last value is retained in the dictionary. \n",
    "# but this is okay because we don't care about the 'no pair' ids. \n",
    "types = dict(zip(pd.concat([meta.leftid, meta.rightid]),\n",
    "                 pd.concat([meta.celltype,meta.celltype])))\n",
    "# have a look at a few \n",
    "dict(list(types.items())[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfbd062-5193-4ca3-af31-25118514a620",
   "metadata": {},
   "outputs": [],
   "source": [
    "types_add = dict(zip(pd.concat([meta.leftid, meta.rightid]),\n",
    "                 pd.concat([meta.annotated_name,meta.annotated_name])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efb47e8-4bfa-4e1e-8414-9a9c8e5ecde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and a neuron name dictionary \n",
    "names = dict(zip(pd.concat([meta.leftid, meta.rightid]),\n",
    "                 pd.concat([meta.left_name,meta.right_name])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e75aad-275e-409a-b899-4d5bf1a709c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and a side dictionary \n",
    "sides = dict.fromkeys(meta.leftid, 'left')\n",
    "sides.update(dict.fromkeys(meta.rightid, 'right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5772da4b-8552-4e76-9e7a-9e2066f1a81a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## total input contributed by senses \n",
    "At different steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61191e2-f6df-4167-bb1d-27940da29012",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sensories = [skid for skid, tp in types.items() if tp == 'sensory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c47f90-2304-4726-9848-f1c5b52d5cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that takes in skids as starting points, and calculate the amount of input contributed by those skids, for n steps \n",
    "def generate_steps(skids, ad_inprop, step_number): \n",
    "    # create the inital almost-identity matrix \n",
    "    ini = ad_inprop.copy()\n",
    "    # turn all values to 0, then assign 1 to the sensory ones on the diagonal \n",
    "    for col in ini.columns:\n",
    "        ini[col].values[:] = 0\n",
    "        if col in skids:\n",
    "            ini.loc[col,col] = 1\n",
    "    \n",
    "    steps_fast = []\n",
    "    for i in tqdm(range(step_number)): \n",
    "        # e.g. if step_number is 2, then range(step_number) is [0,1] \n",
    "        # the if i==0 block gives 'how many neurons receive direct* input from skids, as shown in the connectome' \n",
    "        # then the i=1 step gives 'how many neurons receive input from skids, with one neuron in the middle' \n",
    "        if i==0: \n",
    "            # the first step of signal propagation \n",
    "            steps_fast.append(ini@np.linalg.matrix_power(ad_inprop, 1))\n",
    "            steps_fast[-1].columns = steps_fast[-1].index.copy()\n",
    "        else: \n",
    "            # multiply the last result  with ad_inprop \n",
    "            steps_fast.append(steps_fast[-1]@ad_inprop)\n",
    "    \n",
    "    return steps_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c47cba-a366-4ec7-bef0-f0fbb3a4d62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = generate_steps(sensories, ad_inprop, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103adfdc-7c2a-4904-904d-2a427441ac80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sum the first n steps \n",
    "def add_steps(steps, n): \n",
    "    # n must be 1 or larger \n",
    "    m = steps[0].copy()\n",
    "    # the first step of signal propagation \n",
    "    if n==1: \n",
    "        return m\n",
    "    else: \n",
    "        for i in range(n-1): \n",
    "            m = m + steps[i+1]\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f47eee-a392-400e-9fcc-f0da41c3118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we should exclude the sensory neurons in the columns, and non-sensory neurons in the rows (since the values are 0 anyway) \n",
    "not_sensory = [types[idx]!='sensory' for idx in steps[0].index]\n",
    "sensory = [types[idx]=='sensory' for idx in steps[0].index]\n",
    "steps_nosense = []\n",
    "for m in steps: \n",
    "    m.columns = m.index\n",
    "    steps_nosense.append(m.loc[sensory,not_sensory])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcbaeca-0379-4458-912c-b0fe1a0898d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 14\n",
    "stepsn = add_steps(steps_nosense, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a573f822-6e16-49e5-b539-b703b0ce345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stepsn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b96e083-7884-4a0d-a1f7-dc7fa5fba91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(stepsn.unstack() == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e06343-228a-4230-8413-da506f605c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(stepsn.unstack() == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd81ac1-2a1b-4580-9d39-564ff1f88f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "430*2522"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d748e97c-8fe7-4b19-b381-3429d1fc706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "220360/1084460"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985e1ab6-e394-447a-acac-1463dc78c185",
   "metadata": {
    "tags": []
   },
   "source": [
    "# sensory hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9da03f0-5149-4dee-ac44-5b78b8a109dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage, cut_tree, leaves_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd31965-bfea-49bc-a3e9-1210444826f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by types_add \n",
    "stepsnt = stepsn.T\n",
    "stepsnt['type_additional'] = stepsnt.index.map(types_add)\n",
    "\n",
    "# Split the DataFrame into two parts: one with 'no official annotation' and the other without\n",
    "stepsnt_no_official_annotation = stepsnt[stepsnt['type_additional'] == 'no official annotation']\n",
    "stepsnt_other_types = stepsnt[stepsnt['type_additional'] != 'no official annotation']\n",
    "\n",
    "# Group by the 'type' column and sum the values for the other types\n",
    "result_other_types = stepsnt_other_types.groupby('type_additional').sum()\n",
    "\n",
    "# Concatenate the two DataFrames back together\n",
    "result = pd.concat([stepsnt_no_official_annotation, result_other_types])\n",
    "result = result.drop(['type_additional'], axis = 1).T\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5f4dfe-c1a8-4160-8dca-18e5705e57d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_data = linkage(result, method='ward', metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35009548-e493-45cc-90a7-077df000550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make labels \n",
    "# label by names \n",
    "labs = [names[skid] for skid in stepsn.index]\n",
    "# make colours \n",
    "name_typeadd_dict = dict([(names[skid], types_add[skid]) for skid in stepsn.index])\n",
    "sens_typeadd = set(name_typeadd_dict.values())\n",
    "# colour dictionary \n",
    "sens_typeadd_col = dict(zip(sens_typeadd, \n",
    "                            sns.color_palette('husl',len(sens_typeadd)).as_hex()))\n",
    "sens_typeadd_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a54e7d9-6776-4ea1-9821-2bb9b1cc3104",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (35,10), dpi = 400)\n",
    "\n",
    "dendrogram(linkage_data, labels = labs, color_threshold = 1.8)\n",
    "\n",
    "# add colours to labels \n",
    "ax = plt.gca()\n",
    "xlbls = ax.get_xmajorticklabels()\n",
    "for lbl in xlbls:\n",
    "    lbl.set_color(sens_typeadd_col[name_typeadd_dict[lbl.get_text()]])\n",
    "\n",
    "# add legend \n",
    "# Create a list of patches\n",
    "patches = [mpatches.Patch(color=color, label=label) for label, color in sens_typeadd_col.items()]\n",
    "# Create a legend from these patches\n",
    "plt.legend(handles=patches, loc='upper left') \n",
    "\n",
    "plt.title('Hierarchical clustering of output profile of sensory neurons')\n",
    "\n",
    "plt.savefig('/Users/yijieyin/Downloads/larva_sensory_hclust.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50ed8e4-7d28-459b-94e1-f49d4d355172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make clusters \n",
    "cl = cut_tree(linkage_data, height=1.8)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e744589-4264-4fb6-99b5-9835b430cb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preset order of 'skid'\n",
    "preset_order = stepsn.index[leaves_list(linkage_data)]\n",
    "\n",
    "def plot_activation(input_df):\n",
    "    # Takes a dataframe of columns `skid` and `activation` \n",
    "    \n",
    "    # Create template dataframe with preset skid order\n",
    "    template_df = pd.DataFrame({'skid': preset_order})\n",
    "    \n",
    "    # Map sense onto input dataframe\n",
    "    input_df.columns = ['skid','activation']\n",
    "    input_df.loc[:,'sense'] = input_df['skid'].map(types_add)\n",
    "    \n",
    "    # Merge input dataframe onto template dataframe to maintain skid order\n",
    "    df = template_df.merge(input_df, on='skid', how='left')\n",
    "\n",
    "    g = sns.catplot(\n",
    "        data=df, kind=\"bar\",\n",
    "        x=\"skid\", y=\"activation\", row=\"sense\", hue = 'sense',\n",
    "        height=2, aspect=10, \n",
    "        sharex=False, \n",
    "        palette = sens_typeadd_col\n",
    "    )\n",
    "\n",
    "    # Remove x-tick labels and y-axis labels\n",
    "    g.set(xticklabels=[], ylabel='', xlabel = '')\n",
    "\n",
    "    # Set the facet titles, adjust their size and position\n",
    "    for ax in g.axes.flatten():  \n",
    "        ax.set_title(ax.get_title(), fontsize='xx-large', y = 0.85)\n",
    "\n",
    "    # Adjust vertical spacing \n",
    "    g.fig.subplots_adjust(hspace=0.1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee9b699-0ccd-4429-8340-93c5774be594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859f6c55-7bc8-4978-ac92-9d611a04952d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62c396e6-f501-4e9f-a45d-d79207963a6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ANN\n",
    "## guidance by GPT-4\n",
    "**Preparing the data**: Make sure you have the fruit fly connectome dataset ready in a format that can be easily processed by PyTorch. Convert it to a tensor if it isn't already in that format.\n",
    "\n",
    "**Building the neural network**: Since you're not confident about creating a neural network from scratch, let's start with a simple feedforward network. You can use the torch.nn.Module class to define your network architecture. Here's a basic example:\n",
    "\n",
    "```\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "```\n",
    "Customize the input_size, hidden_size, and output_size according to your needs. This is just an example to get you started. In practice, you might need to adjust the architecture, add more layers, or change the activation functions to better model the fruit fly connectome.\n",
    "\n",
    "Activation maximization: The goal of activation maximization is to find the input that maximally activates a specific neuron or layer in the network. The general approach is to perform gradient ascent on the input, optimizing it to maximize the activation of the selected neuron(s).\n",
    "Here's an outline of how to do this with PyTorch:\n",
    "\n",
    "a. Choose the neuron(s) you want to maximize the activation for.\n",
    "\n",
    "b. Initialize a random input tensor (with the same dimensions as your network's input) and set requires_grad=True to enable gradient computation:\n",
    "\n",
    "```input_tensor = torch.randn((1, input_size), requires_grad=True)```\n",
    "c. Define a loss function that represents the activation of the chosen neuron(s). You can use the output of the network for this purpose. For example, if you want to maximize the activation of neuron i in the output layer:\n",
    "\n",
    "```\n",
    "output = model(input_tensor)\n",
    "loss = -output[0, i]  # Negative sign because we want to maximize activation\n",
    "```\n",
    "d. Compute gradients using backpropagation and update the input tensor using gradient ascent:\n",
    "\n",
    "```\n",
    "optimizer = torch.optim.SGD([input_tensor], lr=learning_rate)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "```\n",
    "e. Repeat steps (c) and (d) for a certain number of iterations or until the loss converges.\n",
    "\n",
    "Interpreting results: Once the optimization is complete, analyze the input tensor that maximizes the activation of your chosen neuron(s). This should give you insights into what kind of stimuli activate the selected neuron(s) the most.\n",
    "Remember to adjust the neural network architecture and parameters based on your specific problem and dataset. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7309d1dc-a63c-474f-b857-9ba7030d7de9",
   "metadata": {},
   "source": [
    "## building the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f131759-6445-4da1-b6ed-b932dec203f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example neural network\n",
    "# class SimpleNetwork(nn.Module): \n",
    "#     def __init__(self, input_side, hidden_size, output_siz): \n",
    "#         super(SimpleNetwork, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "#         self.relu = nn.ReLU() \n",
    "#         self.fc2 = nn.Linear(hidden_size, output_size) \n",
    "    \n",
    "#     def forward(self, x): \n",
    "#         x = self.fc1(x) \n",
    "#         x = self.relu(x) \n",
    "#         x = self.fc2(x) \n",
    "#         return x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a137665-f225-4bf3-9686-91eca93f3e9c",
   "metadata": {},
   "source": [
    "`super()` is a built-in Python function used to call the base class (in this case, **nn.Module**) constructor. This is necessary because the base class constructor sets up some internal mechanisms that make the class work with PyTorch. By calling `super(SimpleNetwork, self).__init__()`, you ensure that the base class constructor is properly executed before adding any custom functionality to your `SimpleNetwork` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8572ef2e-f711-4127-81c0-435b77736328",
   "metadata": {},
   "source": [
    "### two layered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90265a9b-da6b-4e1a-8dfb-01ef263b881d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNetwork(nn.Module): \n",
    "    def __init__(self, input_size, output_size, weights_tensor): \n",
    "        super(TwoLayerNetwork, self).__init__() \n",
    "        # fully connected layer \n",
    "        self.fc1 = nn.Linear(input_size, output_size, bias=False) \n",
    "        self.fc1.weight = nn.Parameter(weights_tensor) \n",
    "#         self.relu = nn.ReLU() \n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x): \n",
    "        x = self.fc1(x) \n",
    "        # make x between 0 and 1 \n",
    "        x = torch.clamp(x, min=0, max=1)\n",
    "#         x = self.relu(x) \n",
    "#         x = self.sigmoid(x)  # Add the Sigmoid activation function to make outputs between 0 and 1 \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29725c3e-c5f1-42e4-90e1-3389792a90e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn into a tensor \n",
    "numpy_array = stepsn.to_numpy()\n",
    "weights_tensor = torch.from_numpy(numpy_array).float().t()\n",
    "weights_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502da4a9-30fb-47b4-927d-ab700bbc3832",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = stepsn.shape[0]\n",
    "output_size = stepsn.shape[1]\n",
    "twol_model = TwoLayerNetwork(input_size, output_size, weights_tensor) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1104060c-5c81-41e6-8120-4dbea2cae63f",
   "metadata": {},
   "source": [
    "###Â multi-layered (unroll through time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86aac2f-0702-4b22-a714-b6096ccc7343",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayeredNetwork(nn.Module): \n",
    "    # this network unrolls the connectome through time: \n",
    "    # first layer is sensory to non-sensory \n",
    "    # later layers are non-sensory to non-sensory \n",
    "    # there will likely be a problem with vanishing/exploding gradient the more layers you have \n",
    "    def __init__(self, sensory_weights, weights_nonsense, num_layers=2, threshold = 0.01): \n",
    "        super(MultilayeredNetwork, self).__init__()\n",
    "        self.sensory_weights = torch.nn.parameter.Parameter(sensory_weights)\n",
    "        self.weights = torch.nn.parameter.Parameter(weights_nonsense.T) \n",
    "        self.num_layers = num_layers\n",
    "        self.threshold = threshold\n",
    "        self.activations = []  # List to save the activations of the middle layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.activations = []  # Clear the list at the start of each forward pass\n",
    "\n",
    "        # first multiply by weights \n",
    "        x = x@self.sensory_weights.T\n",
    "        # you can write your own activation function \n",
    "        # this is a thresholded ReLU \n",
    "        x = torch.where(x >= self.threshold, x, 0)\n",
    "        self.activations.append(x)  # Save the activations after the first layer\n",
    "\n",
    "        \n",
    "        for i in range(self.num_layers-1):\n",
    "            x = x@self.weights \n",
    "            x = torch.where(x >= self.threshold, x, 0)\n",
    "            self.activations.append(x)  # Save the activations after each layer\n",
    "\n",
    "#         # make x between 0 and 1 \n",
    "#         x = torch.clamp(x, min=0, max=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f00584-f8c2-4f90-93bb-f13645e9296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_sensory = np.array([types[idx] == 'sensory' for idx in ad_inprop.index])\n",
    "# from sensory to non-sensory \n",
    "# input is in the COLUMNS - column vectors \n",
    "sensory_weights = torch.from_numpy(ad_inprop.loc[is_sensory, ~is_sensory].to_numpy()).float().t()\n",
    "# from non-sensory to non-sensory\n",
    "weights_nonsense = torch.from_numpy(ad_inprop.loc[~is_sensory, ~is_sensory].to_numpy()).float().t()\n",
    "ml_model = MultilayeredNetwork(sensory_weights, weights_nonsense, num_layers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec98947-e021-4dd8-954f-9b5c1f40e68a",
   "metadata": {},
   "source": [
    "### activation maximisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da96223-f8a5-4a7b-bf5e-ec926d5e195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose which neuron to maximise the activation for \n",
    "selected_indices = [idx for idx, key in enumerate(stepsn.columns) if types_add[key] == 'DAN-j1']\n",
    "selected_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687a5520-618d-41a2-8914-103928b316de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_input_size(model):\n",
    "        # only the first iteration is run \n",
    "        for param in model.parameters():\n",
    "            return param.shape[1]\n",
    "        \n",
    "def activation_maximisation(model, target_indices, layer_index = None, input_tensor = None, \n",
    "                            num_iterations = 3000, learning_rate = 0.01, \n",
    "                            regularisation_lambda = 0.1, regularisation_norm_order = 0.5, \n",
    "                            use_tqdm = True, print_output = True): \n",
    "    \"\"\"\n",
    "    For now intended to work with one input_tensor at a time. \n",
    "    \"\"\"\n",
    "\n",
    "    if input_tensor is None: \n",
    "        input_size = get_input_size(model)\n",
    "        input_tensor = torch.rand((1, input_size), requires_grad = True)\n",
    "    \n",
    "    output_before = model(input_tensor)\n",
    "    if print_output: \n",
    "        print(\"Output before optimization:\", torch.mean(output_before[0,target_indices]).item())\n",
    "\n",
    "    optimizer = torch.optim.Adam([input_tensor], lr=learning_rate)\n",
    "\n",
    "    losses = []\n",
    "    act_loss = []\n",
    "    reg_losses = []\n",
    "    iteration_range = range(num_iterations)\n",
    "    if use_tqdm:\n",
    "        iteration_range = tqdm(iteration_range)\n",
    "    for _ in iteration_range:\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        output = model(input_tensor)\n",
    "        \n",
    "        # Check if the model has an 'activations' attribute\n",
    "        if hasattr(model, 'activations'): \n",
    "            # If so, use the activations of the specified layer to compute the activation loss\n",
    "            # Negative sign because we want to maximize activation \n",
    "            if layer_index is not None:\n",
    "                activation_loss = -torch.mean(model.activations[layer_index][0,target_indices])\n",
    "            else: \n",
    "                # mean level of activation for all neurons for all times \n",
    "                activation_loss = -torch.mean(ml_model.activations[selected_indices,:])\n",
    "        else:\n",
    "            # If not, use the final output of the model to compute the activation loss\n",
    "            activation_loss = -torch.mean(output[0,target_indices])\n",
    "\n",
    "        # torch.norm(p = 1): sum of the absolute values of the tensor's elements - l1 regularisation \n",
    "        # dividing by input_tensor.shape[1] hopefully normalises this term to be similar to activation_loss \n",
    "        regularisation_loss = regularisation_lambda * torch.norm(input_tensor, p=regularisation_norm_order) / input_tensor.shape[1]\n",
    "\n",
    "        # need to make sure that the two terms are on the same scale \n",
    "#         loss =  activation_loss + regularisation_loss + 1\n",
    "        # possible alternative loss function: \n",
    "#         loss = regularisation_loss / -activation_loss\n",
    "        loss = activation_loss + regularisation_loss\n",
    "#         loss = regularisation_loss*(1+activation_loss)\n",
    "        act_loss.append(activation_loss.item())\n",
    "        reg_losses.append(regularisation_loss.item())\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    # print(\"Input tensor after optimization:\", input_tensor)\n",
    "    output_after = model(input_tensor)\n",
    "    # note that this isn't necessarily in the loss function, if you care about the activation of intermediate neurons. \n",
    "    if print_output: \n",
    "        print(\"Output after optimization:\", torch.mean(output_after[0,target_indices]).item())\n",
    "    \n",
    "    return (input_tensor.detach().numpy(), output_after.detach().numpy(), act_loss, reg_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1fb46f-8720-4e56-9e50-3dccd1d37b32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimised_in, out, act_loss, reg_loss = activation_maximisation(ml_model, selected_indices, layer_index = 2,\n",
    "                                                                num_iterations = 400, regularisation_norm_order = 0.5, \n",
    "                                                               regularisation_lambda = 0.005, learning_rate = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac76d937-d01c-47a8-b36b-cffa91c43408",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(act_loss, reg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e91a2d-56d2-4595-b238-5679470b8def",
   "metadata": {},
   "source": [
    "###Â multi-layered with timed input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4236ddf5-b33c-4c2f-95e8-6971c4267f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayeredNetwork(nn.Module): \n",
    "    # this network unrolls the connectome through time: \n",
    "    # first layer is sensory to non-sensory \n",
    "    # later layers are of the shape all_neurons * non-sensory \n",
    "    # each layer takes their own sensory input \n",
    "    # there will likely be a problem with vanishing/exploding gradient the more layers you have \n",
    "    def __init__(self, sensory_weights, weights_nonsense, num_layers=2, threshold = 0.01): \n",
    "        super(MultilayeredNetwork, self).__init__()\n",
    "        self.sensory_weights = torch.nn.parameter.Parameter(sensory_weights)\n",
    "        # output * input (both sensory and non-sensory, SENSORY FIRST) \n",
    "        self.weights = torch.nn.parameter.Parameter(torch.concat([sensory_weights, weights_nonsense], axis = 1)) \n",
    "        self.num_layers = num_layers\n",
    "        self.threshold = threshold\n",
    "        self.activations = []  # List to save the activations of the middle layers\n",
    "    \n",
    "    def forward(self, inthroughtime):\n",
    "        # inthroughtime is like an 'image': neurons * timesteps \n",
    "        self.activations = []  # Clear the list at the start of each forward pass\n",
    "        inthroughtime = torch.tanh(inthroughtime)\n",
    "#         inthroughtime = torch.sigmoid(inthroughtime)\n",
    "        \n",
    "        # first multiply by weights \n",
    "        x = self.sensory_weights@inthroughtime[:,0]\n",
    "        # you can write your own activation function \n",
    "        # this is a thresholded ReLU \n",
    "        x = torch.where(x >= self.threshold, x, 0)\n",
    "        x = torch.tanh(x)\n",
    "#         x = torch.sigmoid(x)\n",
    "        self.activations.append(x)  # Save the activations after the first layer\n",
    "        \n",
    "        for i in range(self.num_layers-1):\n",
    "            x = self.weights @ torch.concat([inthroughtime[:,i+1], x])\n",
    "            x = torch.where(x >= self.threshold, x, 0)\n",
    "#             # limit the range between 0 and 1 \n",
    "            x = torch.tanh(x)\n",
    "#             x = torch.sigmoid(x)\n",
    "            self.activations.append(x)  # Save the activations after each layer\n",
    "        \n",
    "        self.activations = torch.stack(self.activations, dim=1)\n",
    "        return self.activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1109ed-60a1-42f2-9b0f-e973be03c3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_sensory = np.array([types[idx] == 'sensory' for idx in ad_inprop.index])\n",
    "# from sensory to non-sensory \n",
    "# input is in the COLUMNS - column vectors \n",
    "sensory_weights = torch.from_numpy(ad_inprop.loc[is_sensory, ~is_sensory].to_numpy()).float().t()\n",
    "# from non-sensory to non-sensory\n",
    "weights_nonsense = torch.from_numpy(ad_inprop.loc[~is_sensory, ~is_sensory].to_numpy()).float().t()\n",
    "\n",
    "ml_model = MultilayeredNetwork(sensory_weights, weights_nonsense, num_layers=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133db60b-9bc1-4e69-97b4-c3e61d67be9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test \n",
    "input_tensor = torch.rand((sensory_weights.shape[1], ml_model.num_layers))\n",
    "out = ml_model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04820c83-1cda-4748-89d2-04029376d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose which neuron to maximise the activation for \n",
    "selected_indices = [idx for idx, key in enumerate(stepsn.columns) if types_add[key] == 'DAN-j1']\n",
    "selected_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c502b82-698a-410f-b0a5-169b786cb392",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_input_size(model):\n",
    "        # only the first iteration is run \n",
    "        for param in model.parameters():\n",
    "            return param.shape[1]\n",
    "        \n",
    "def activation_maximisation(model, target_indices, input_tensor = None, \n",
    "                            num_iterations = 3000, learning_rate = 0.01, \n",
    "                            regularisation_lambda = 0.1, \n",
    "                            regularisation_norm_order = 0.5, \n",
    "                            use_tqdm = True, print_output = True): \n",
    "    \"\"\"\n",
    "    For now intended to work with one input_tensor at a time. \n",
    "    \"\"\"\n",
    "\n",
    "    if input_tensor is None: \n",
    "        input_size = get_input_size(model)\n",
    "        input_tensor = torch.rand((input_size, model.num_layers), requires_grad = True)\n",
    "    \n",
    "    output_before = model(input_tensor)\n",
    "    if print_output: \n",
    "        print(\"Output before optimization:\", torch.mean(output_before[selected_indices,:]).item())\n",
    "\n",
    "    optimizer = torch.optim.Adam([input_tensor], lr=learning_rate)\n",
    "\n",
    "    losses = []\n",
    "    act_loss = []\n",
    "    reg_losses = []\n",
    "    iteration_range = range(num_iterations)\n",
    "    if use_tqdm:\n",
    "        iteration_range = tqdm(iteration_range)\n",
    "    for _ in iteration_range:\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        output = model(input_tensor)\n",
    "        \n",
    "        # Check if the model has an 'activations' attribute\n",
    "        if hasattr(model, 'activations'): \n",
    "            # If so, use the activations of the specified layer to compute the activation loss\n",
    "            # Negative sign because we want to maximize activation \n",
    "            # max level of activation for all neurons for all times \n",
    "            activation_loss = -torch.mean(model.activations[selected_indices,:])\n",
    "\n",
    "        # torch.norm(p = 1): sum of the absolute values of the tensor's elements - l1 regularisation \n",
    "        regularisation_loss = regularisation_lambda * (torch.norm(input_tensor, p = 1) + torch.norm(input_tensor, p = 2))\n",
    "        \n",
    "        # or specify regularisation order \n",
    "        # dividing by input_tensor.shape[1] hopefully normalises this term to be similar to activation_loss \n",
    "#         regularisation_loss = regularisation_lambda * torch.norm(input_tensor, p=regularisation_norm_order) / input_tensor.shape[0]\n",
    "\n",
    "        # need to make sure that the two terms are on the same scale \n",
    "#         loss =  activation_loss + regularisation_loss + 1\n",
    "        # possible alternative loss function: \n",
    "#         loss = regularisation_loss / -activation_loss\n",
    "        loss = activation_loss + regularisation_loss\n",
    "#         loss = regularisation_loss*(1+activation_loss)\n",
    "        act_loss.append(activation_loss.item())\n",
    "        reg_losses.append(regularisation_loss.item())\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    # print(\"Input tensor after optimization:\", input_tensor)\n",
    "    output_after = model(input_tensor)\n",
    "    # note that this isn't necessarily in the loss function, if you care about the activation of intermediate neurons. \n",
    "    if print_output: \n",
    "        print(\"Output after optimization:\", torch.mean(output_after[selected_indices,:]).item())\n",
    "    \n",
    "    return (input_tensor.detach().numpy(), output_after.detach().numpy(), act_loss, reg_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7cba52-93fb-4e8a-943c-5dc8e17e93d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(activation_loss, regularisation_loss): \n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    ax[0].plot(activation_loss) \n",
    "    ax[0].set_title('Activation loss')\n",
    "    ax[1].plot(regularisation_loss)\n",
    "    ax[1].set_title('Regularisation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e337379-d027-4940-8406-90707350bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimised_in, out, act_loss, reg_loss = activation_maximisation(ml_model, selected_indices, \n",
    "                                                                input_tensor = torch.rand((get_input_size(ml_model), ml_model.num_layers), requires_grad = True),\n",
    "                                                                num_iterations = 200, \n",
    "                                                               regularisation_lambda = 0.0001, learning_rate = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb324d08-f97a-4832-8d98-55b3d39034e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(act_loss, reg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84f495a-961c-41c0-b3a1-1410ee7b2e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import HoverTool, ColumnDataSource\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de40fdd-1d87-4d74-b0fc-afa7e0e293cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_time = {'values':optimised_in.tolist(), \n",
    "            'axis': [list(range(1,optimised_in.shape[1]+1))]*optimised_in.shape[0],\n",
    "           'skid': ad_inprop.index[is_sensory]}\n",
    "act_time['name'] = act_time['skid'].map(names)\n",
    "act_time['type'] = act_time['skid'].map(types)\n",
    "act_time['type_add'] = act_time['skid'].map(types_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110f2a36-4645-4e22-a3c7-f7c5a831a7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_opts = dict(width=1000, height=800)\n",
    "hover_opts = dict(\n",
    "    tooltips=[('type_add', '@type_add'), ('Name', '@name'), ('skid', '@skid')],\n",
    "    show_arrow=False,\n",
    "    line_policy='next',\n",
    ")\n",
    "line_opts = dict(\n",
    "    line_width=2, line_alpha=0.7,hover_line_alpha=1.0,\n",
    "    source=act_time,\n",
    ")\n",
    "\n",
    "act_plot = figure(tools=[HoverTool(**hover_opts), TapTool()], **figure_opts)\n",
    "act_plot.multi_line(xs='axis', ys='values', **line_opts)\n",
    "act_plot.xaxis.axis_label = \"Timestep\"\n",
    "act_plot.yaxis.axis_label = \"Activation\"\n",
    "\n",
    "show(act_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c481b37-abbd-4bfc-904c-b1aca6a01d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_time = {'values':out.tolist(), \n",
    "            'axis': [list(range(1,out.shape[1]+1))]*out.shape[0],\n",
    "           'skid': ad_inprop.index[~is_sensory]}\n",
    "act_time['name'] = act_time['skid'].map(names)\n",
    "act_time['type'] = act_time['skid'].map(types)\n",
    "act_time['type_add'] = act_time['skid'].map(types_add)\n",
    "\n",
    "figure_opts = dict(width=1000, height=800)\n",
    "hover_opts = dict(\n",
    "    tooltips=[('type_add', '@type_add'), ('Name', '@name'), ('skid', '@skid')],\n",
    "    show_arrow=False,\n",
    "    line_policy='next',\n",
    ")\n",
    "line_opts = dict(\n",
    "    line_width=2, line_alpha=0.4,hover_line_alpha=1.0,\n",
    "    source=act_time,\n",
    ")\n",
    "\n",
    "act_plot = figure(tools=[HoverTool(**hover_opts), TapTool()], **figure_opts)\n",
    "act_plot.multi_line(xs='axis', ys='values', **line_opts)\n",
    "act_plot.xaxis.axis_label = \"Timestep\"\n",
    "act_plot.yaxis.axis_label = \"Activation\"\n",
    "\n",
    "show(act_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c334fa-4048-470b-a6e8-898f10f7a80b",
   "metadata": {},
   "source": [
    "Can't do this in the optimisation loop: \n",
    "`input_tensor.data = torch.sigmoid(input_tensor.data)  # Apply the Sigmoid function after each update to make sure it's between 0 and 1`\n",
    "\n",
    "GPT4: When you apply the Sigmoid function to the input tensor within the loop, you create a new tensor that's not a leaf tensor. Leaf tensors are the ones that you can optimize directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be92a21-d60b-4296-8d95-99ed49253277",
   "metadata": {},
   "source": [
    "## activation maximisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f04ca7a-8b15-4ab5-b041-2e7443201025",
   "metadata": {},
   "source": [
    "### all neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b6a832-c943-4907-99c2-4da0f5becbdb",
   "metadata": {},
   "source": [
    "negative input: \n",
    "maybe not long enough? \n",
    "maybe loss function? \n",
    "remove regularisation and see if you still get negative numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1d3a6a-dd3e-463c-b996-6e3ee4de1995",
   "metadata": {},
   "outputs": [],
   "source": [
    "allins = dict()\n",
    "allouts = dict()\n",
    "act_losses = dict()\n",
    "reg_losses = dict()\n",
    "\n",
    "for idx, key in enumerate(tqdm(stepsn.columns)): \n",
    "    selected_indices = idx\n",
    "    optimised_in, out, act_loss, reg_loss = activation_maximisation(twol_model, selected_indices, layer_index = 2,\n",
    "                                                                num_iterations = 800, regularisation_norm_order = 0.5, \n",
    "                                                               regularisation_lambda = 0.1, learning_rate = 0.05, \n",
    "                                                                   use_tqdm = False, print_output = False)\n",
    "    allins[key] = optimised_in[0]\n",
    "    allouts[key] = out[0]\n",
    "    act_losses[key] = act_loss\n",
    "    reg_losses[key] = reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a1d1a9-c910-44c0-aed8-7f4f0d181fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ins = pd.DataFrame(allins)\n",
    "all_ins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab08411d-9aba-4458-818d-40ee5f6797dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ins.index = stepsn.index\n",
    "all_ins.fillna(0, inplace=True)\n",
    "all_ins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458a3a61-5802-4e18-8f41-918dabd9d7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ins.to_csv('/Users/yijieyin/Downloads/larva/two_layer_activation_maximisation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b05a92-51ed-4780-86f9-6df23f3a4cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ins.unstack().hist(bins = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb83808-e5e5-4382-aa75-b780f9b7ee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "# !pip install bokeh\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource, IndexFilter, Spinner, TextInput, CustomJS, Select, LassoSelectTool, Div, Range1d\n",
    "from bokeh.palettes import Spectral10\n",
    "from bokeh.io import output_file, show, save\n",
    "from bokeh.layouts import layout, column, row\n",
    "from bokeh import events\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aac2ae-9daf-44e0-aa60-5b25898d8208",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP()\n",
    "\n",
    "# dimensions to reduce are in the columns \n",
    "in_from_senses_data = all_ins.T.values\n",
    "embedding = reducer.fit_transform(in_from_senses_data)\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc9217a-acdb-4d4f-aced-e2071226606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "typecolourdict = pd.read_csv('/Users/yijieyin/Downloads/larva/type_colour_dict.csv')\n",
    "typecolourdict = dict(zip(typecolourdict.type_name, typecolourdict.colour))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7530f5bc-2c00-4712-b65f-4f2398c6bbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "all_sensory_bk = pd.concat([all_ins.T,\n",
    "           pd.DataFrame(embedding, columns=('x', 'y'), index = all_ins.columns)], axis = 1)\n",
    "all_sensory_bk = all_sensory_bk.round(3)\n",
    "# add meta info \n",
    "all_sensory_bk['neuron_name'] = all_sensory_bk.index.map(names)\n",
    "all_sensory_bk['type_name'] = all_sensory_bk.index.map(types) \n",
    "all_sensory_bk['colour'] = all_sensory_bk.type_name.map(typecolourdict)\n",
    "all_sensory_bk['type_add'] = all_sensory_bk.index.map(types_add)\n",
    "all_sensory_bk['side'] = all_sensory_bk.index.map(sides)\n",
    "all_sensory_bk['skid'] = all_sensory_bk.index\n",
    "all_sensory_bk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297be827-2852-4e93-9e10-2855d26d6fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "allin_wmodal = all_ins.copy()\n",
    "allin_wmodal['modality'] = [types_add[skid] for skid in allin_wmodal.index]\n",
    "allin_wmodal['name'] = allin_wmodal.index.map(names)\n",
    "# remove things inside brackets to make x axis tick labels smaller \n",
    "# allin_wmodal['name'] = [re.sub('\\(.*\\)', '', x) for x in allin_wmodal.name]\n",
    "\n",
    "# Create a dictionary of empty ColumnDataSources, one for each modality\n",
    "# this specified the sequence of modalities to be plotted \n",
    "modalities = ['thermo-cold','thermo-warm','respiratory','visual','olfactory','enteric','gustatory-pharyngeal','gustatory-external']\n",
    "sources = {modality: ColumnDataSource(data=dict(index=allin_wmodal.index[allin_wmodal.modality.isin([modality])], \n",
    "                                                top=[0]*len(allin_wmodal.index[allin_wmodal.modality.isin([modality])]))) for modality in modalities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717d9d5d-4b95-435f-a285-b0e2ef8a15b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot for each modality\n",
    "bar_plots = []\n",
    "for modality in modalities:\n",
    "    this_mod = allin_wmodal[allin_wmodal.modality.isin([modality])]\n",
    "    if modality in ['thermo-cold','thermo-warm']: \n",
    "        plot = figure(x_range = list(this_mod.index), title=modality, tools='box_select,reset, wheel_zoom', \n",
    "                     width = 200, height = 200)\n",
    "    elif modality in ['visual','respiratory']: \n",
    "        plot = figure(x_range = list(this_mod.index), title=modality, tools='box_select,reset, wheel_zoom', \n",
    "                     width = 400, height = 200)\n",
    "    else: \n",
    "        plot = figure(x_range = list(this_mod.index), title=modality, tools='box_select,reset, wheel_zoom', \n",
    "                     width = 1300, height = 150)\n",
    "    plot.vbar(x='index', top='top', source=sources[modality], width=0.5)\n",
    "    # rotate 45 degrees\n",
    "    plot.xaxis.major_label_orientation = math.pi/4\n",
    "    plot.xaxis.axis_label_text_font_size = \"2pt\"\n",
    "#     plot.y_range = Range1d(0, 0.3)\n",
    "    bar_plots.append(plot)\n",
    "\n",
    "# scatter plot \n",
    "datasource = ColumnDataSource(all_sensory_bk)\n",
    "s2 = ColumnDataSource(all_sensory_bk)\n",
    "\n",
    "plot = figure(\n",
    "    title='UMAP projection of individual inputs',\n",
    "    width=500,\n",
    "    height = 400,\n",
    "#     plot_height=600,\n",
    "    tools=('pan, wheel_zoom, reset, tap, lasso_select')\n",
    ")\n",
    "\n",
    "renderer = plot.circle(\n",
    "    'x',\n",
    "    'y',\n",
    "    source=datasource,\n",
    "    color='colour',\n",
    "    legend_field = 'type_name',\n",
    "    line_alpha=0.6,\n",
    "    fill_alpha=0.6,\n",
    "    size=4, \n",
    "    selection_alpha=1,\n",
    "    nonselection_alpha=0.05\n",
    ")\n",
    "# add a transparent one for selective hovering \n",
    "renderer2 = plot.circle(\n",
    "    'x',\n",
    "    'y',\n",
    "    source=s2,\n",
    "    alpha=0  # set alpha to 0 to make this renderer invisible\n",
    ")\n",
    "\n",
    "div = Div(text=\"Selected neuron name: \", width = 400)\n",
    "type_select = Select(title=\"Select Type:\", \n",
    "                     options=['all'] + list(all_sensory_bk['type_name'].unique()), \n",
    "                     value=\"\")\n",
    "\n",
    "# Define a JavaScript callback function for the widget\n",
    "type_select_callback = CustomJS(args=dict(source1=datasource, source2=s2, select=type_select, div = div), code=\"\"\"\n",
    "    const selected_type = select.value;\n",
    "    const indices = [];\n",
    "    const names = []; \n",
    "    const data1 = source1.data;\n",
    "    const data2 = source2.data;\n",
    "    const typeName = data1.type_name;\n",
    "    const allNames = data1.neuron_name; \n",
    "    for (let i = 0; i < typeName.length; i++) {\n",
    "        if (typeName[i] === selected_type || selected_type === \"all\") {\n",
    "            indices.push(i);\n",
    "            if (selected_type !== 'all'){\n",
    "                names.push(allNames[i]); \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    source1.selected.indices = indices;\n",
    "    source1.change.emit();\n",
    "    div.text = 'Selected neuron name: ' + names; \n",
    "    \n",
    "    // Update the data of the second data source to match the selected data\n",
    "    for (let key in data2) {\n",
    "        data2[key] = data1[key].filter((_, i) => indices.includes(i));\n",
    "    }\n",
    "    source2.change.emit();\n",
    "    \n",
    "    \"\"\")\n",
    "type_select.js_on_change('value', type_select_callback)\n",
    "\n",
    "lasso_callback = CustomJS(args=dict(source=datasource, div=div), code=\"\"\"\n",
    "    const indices = source.selected.indices;\n",
    "    const original_indices = indices.map(i => source.data.neuron_name[i]);\n",
    "    const original_indices_string = original_indices.join(\", \");\n",
    "    // Update the text of the Div\n",
    "    div.text = \"Selected Indices: \" + original_indices_string;\n",
    "\"\"\")\n",
    "\n",
    "plot.js_on_event('selectiongeometry', lasso_callback)\n",
    "\n",
    "\n",
    "# Define a JavaScript callback to update 'allin_source' when a circle is selected\n",
    "allin_wmodal_source = ColumnDataSource(allin_wmodal)\n",
    "callback = CustomJS(args=dict(source=datasource, \n",
    "                              sources = sources, \n",
    "                              allin_wmodal = allin_wmodal_source, \n",
    "                             div = div), \n",
    "                    code=\"\"\"\n",
    "    var selected_indices = source.selected.indices;\n",
    "    if (selected_indices.length == 0) {\n",
    "        return;\n",
    "    }    \n",
    "    var selected_skid = source.data['skid'][selected_indices[0]];\n",
    "    var selected_name = source.data['neuron_name'][selected_indices[0]];\n",
    "    div.text = \"Selected neuron skid: \" + selected_skid + '; Neuron name: ' + selected_name;\n",
    "\n",
    "    // update bar plots \n",
    "    for (var modality in sources) {\n",
    "    var source_to_update = sources[modality];\n",
    "\n",
    "    var new_data = [];\n",
    "    for (var i = 0; i < allin_wmodal.data['modality'].length; i++) {\n",
    "        if (allin_wmodal.data['modality'][i] === modality) {\n",
    "            new_data.push(allin_wmodal.data[selected_skid][i]);\n",
    "        }\n",
    "    }\n",
    "    console.log(new_data); \n",
    "    source_to_update.data['top'] = new_data;\n",
    "    source_to_update.change.emit();   \n",
    "    }\n",
    "\"\"\")\n",
    "plot.js_on_event(events.Tap, callback)\n",
    "\n",
    "\n",
    "# add text on hover \n",
    "plot.add_tools(HoverTool(renderers=[renderer2], tooltips=\"\"\"\n",
    "<div>\n",
    "    <div><span style='font-size: 15px'>@type_add; @neuron_name; @side </span></div>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "plot.legend.location = \"top_right\"\n",
    "plot.legend.label_text_font_size = '8pt'\n",
    "\n",
    "# Combine all plots together\n",
    "layout = column(row(column(*bar_plots[0:2]), plot, column(*bar_plots[2:4]), column(type_select, div)), *bar_plots[4:])\n",
    "# layout = column(row(plot, *bar_plots[4:]), *bar_plots[0:4])\n",
    "\n",
    "# Show the result\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6e22a3-d32e-46b9-a073-2bd944481d8c",
   "metadata": {},
   "source": [
    "lateralised PED and medial lobe (cvijk, MBIN-l - long term memory? SERT wrap around one of the lobes necessary & sufficient for long term memory)\n",
    "vertical lobe not very lateralised (defg) \n",
    "CA not lateralised: OANs are bilateral "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a05ac17-d564-4358-a008-85e69924d663",
   "metadata": {},
   "source": [
    "### hyperparameter exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba774bb7-81df-45f3-bb42-44708134d15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store results\n",
    "activation_losses = []\n",
    "regularisation_losses = []\n",
    "regularisation_norm_orders = []\n",
    "regularisation_lambdas = []\n",
    "optimised_inputs = []\n",
    "\n",
    "# Define line styles for different regularisation_lambda values\n",
    "line_styles = ['-', '--', '-.', ':']\n",
    "\n",
    "# Define colors for different regularisation_norm_order values\n",
    "colors = ['red', 'green', 'blue']\n",
    "\n",
    "# Define the hyperparameters\n",
    "regularisation_norm_order_values = [0.5, 1, 2]\n",
    "regularisation_lambda_values = [0.01, 0.1, 1, 2]\n",
    "\n",
    "# Loop over hyperparameters\n",
    "for regularisation_norm_order in regularisation_norm_order_values: \n",
    "    for regularisation_lambda in regularisation_lambda_values: \n",
    "        # Run the activation maximisation function\n",
    "        op_input, _, act_loss, reg_loss = activation_maximisation(\n",
    "            twol_model, \n",
    "            selected_indices, \n",
    "            layer_index = 1,\n",
    "            num_iterations = 500, \n",
    "            regularisation_norm_order = regularisation_norm_order, \n",
    "            regularisation_lambda = regularisation_lambda, \n",
    "            learning_rate = 0.1\n",
    "        )\n",
    "        \n",
    "        # Store the losses and the hyperparameters\n",
    "        activation_losses.append(act_loss)\n",
    "        regularisation_losses.append(reg_loss)\n",
    "        regularisation_norm_orders.append(regularisation_norm_order)\n",
    "        regularisation_lambdas.append(regularisation_lambda)\n",
    "        optimised_inputs.append(op_input)\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(len(regularisation_norm_order_values), 2, figsize=(15, 5*len(regularisation_norm_order_values)))\n",
    "\n",
    "# Loop over regularisation_norm_order_values\n",
    "for i, regularisation_norm_order in enumerate(regularisation_norm_order_values):\n",
    "    # Activation loss as a function of regularisation_norm_order and regularisation_lambda\n",
    "    for j, losses in enumerate(activation_losses):\n",
    "        if regularisation_norm_orders[j] == regularisation_norm_order:\n",
    "            ax[i, 0].plot(losses, label=f'Lambda: {regularisation_lambdas[j]}', \n",
    "                          linestyle=line_styles[regularisation_lambda_values.index(regularisation_lambdas[j])])\n",
    "    ax[i, 0].set_title(f'Activation loss vs. Iteration (Order: {regularisation_norm_order})')\n",
    "    ax[i, 0].set_xlabel('Iteration')\n",
    "    ax[i, 0].set_ylabel('Activation loss')\n",
    "    ax[i, 0].legend()\n",
    "\n",
    "    # Regularisation loss as a function of regularisation_norm_order and regularisation_lambda\n",
    "    for j, losses in enumerate(regularisation_losses):\n",
    "        if regularisation_norm_orders[j] == regularisation_norm_order:\n",
    "            ax[i, 1].plot(losses, label=f'Lambda: {regularisation_lambdas[j]}', \n",
    "                          linestyle=line_styles[regularisation_lambda_values.index(regularisation_lambdas[j])])\n",
    "    ax[i, 1].set_title(f'Regularisation loss vs. Iteration (Order: {regularisation_norm_order})')\n",
    "    ax[i, 1].set_xlabel('Iteration')\n",
    "    ax[i, 1].set_ylabel('Regularisation loss')\n",
    "    ax[i, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c59596-7b8b-4275-a226-677f3adc2edd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "act = pd.DataFrame({'skid': stepsn.index, \n",
    "                    'activation': optimised_inputs[0][0].detach().numpy()})\n",
    "plot_activation(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4571ab-ae4e-4030-bd3f-0839228dcd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-sensory neuron, outputs \n",
    "nonsenses = out.detach().tolist()[0]\n",
    "plt.hist(nonsenses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d39e30-6e4d-46f0-84ac-a963728198d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input sensory neurons \n",
    "senses = optimised_in.detach().tolist()[0]\n",
    "plt.hist(senses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e5f712-847a-49ea-9dac-d2f8e28a1cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation of skids after gradient descent \n",
    "skid_activation = dict(zip(stepsn.index, senses))\n",
    "skid_activation.update(dict(zip(stepsn.columns, nonsenses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd37f069-cd42-4cdf-a51e-3258127b0a94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# whose activation was maximised? \n",
    "#Â by getting act > 1, removing the output, as the output is between 0.5 and 1 \n",
    "[(types_add[skid], names[skid], skid, act) for skid, act in skid_activation.items() if act > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeb04d1-6582-411a-b88b-42057fc05c26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# which output is co-activated by the activation of the above sensory neurons? \n",
    "[(names[skid], skid, skid_activation[skid]) for skid in stepsn.columns if skid_activation[skid]>0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a864299-c0b5-41d5-8b42-bec6883ccdb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2171d719-fa60-4bfe-b6d9-b38aa8d495bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "857ffe4f-cb91-42c5-b9b7-3d37f3c3be88",
   "metadata": {},
   "source": [
    "# saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db27c051-a46a-45aa-a07d-8ea28d18393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.rand((50, input_size), requires_grad = True)\n",
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e849cf14-7bef-429b-965c-3723e67693e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_gradient(model, samples): \n",
    "    # Check if samples requires gradient, if not, set it\n",
    "    if not samples.requires_grad:\n",
    "        samples.requires_grad_(True)\n",
    "\n",
    "    gradients = []\n",
    "    for i in range(samples.size(0)):  # Loop over each sample\n",
    "        # sample tensor is created as a result of indexing operation on samples tensor (which itself should \n",
    "        # be a leaf tensor), hence it's a non-leaf tensor. So pytorch doesn't automatically remember the gradient \n",
    "        # that's why you need to retain grad \n",
    "        sample = samples[i:i+1]  # Get the current sample\n",
    "        # this is of shape (1,input_size). The first one is taken as the batch dimension\n",
    "        sample.retain_grad()  # Tell PyTorch to store the gradient for this tensor\n",
    "        \n",
    "        model.zero_grad()  # Ensure the model has zero gradients\n",
    "        \n",
    "        out = model(sample)  # Forward pass\n",
    "        # though it's called 'loss', it's average neuron activation \n",
    "        # if the gradients are positive, it means: when input neuron increases in activation, output also does \n",
    "        loss = torch.mean(out[:,selected_indices])  # Compute the loss\n",
    "        loss.backward()  # Compute gradients\n",
    "\n",
    "        gradients.append(sample.grad.squeeze(0).clone())  # Store gradients\n",
    "        # squeeze(0) removes the dimension (in this case the batch one) if the first dimension is 1 \n",
    "    \n",
    "    return torch.stack(gradients)  # Stack the list of tensors into one tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa6773e-3277-4004-8a36-bcc1b727b169",
   "metadata": {
    "tags": []
   },
   "source": [
    "A similar function in tensorflow: \n",
    "```\n",
    "def vanilla_gradient(model, samples):\n",
    "    \"\"\"\n",
    "    Outputs a saliency map, using the Vanilla Gradient algorithm, indicating\n",
    "    which input pixels are important for the class predicted by `model` for\n",
    "    inputs in `samples`\n",
    "    Args:\n",
    "        model: a valid TensorFlow Model whose outputs we want to explain\n",
    "        samples: a Tensor of testing samples with shape (B, H, W, C) which we\n",
    "                 want to explain\n",
    "    Returns:\n",
    "        a 3D tensor with shape (B, H, W) with as many grayscale saliency maps as\n",
    "        inputs in `samples`\n",
    "    \"\"\"\n",
    "    images = tf.Variable(samples, dtype=float)\n",
    "    with tf.GradientTape() as tape:\n",
    "      # pred is two dimensional: one for the particular sample, the other for the probability for that class \n",
    "        pred = tf.nn.softmax(model(images, training=False), axis=-1)\n",
    "        # target_classes = np.argmax(pred.numpy(), axis=-1)\n",
    "        # loss = []\n",
    "        # for i, idx in enumerate(target_classes):\n",
    "        #     loss.append(pred[i:i+1, idx])\n",
    "        # loss = tf.concat(loss, axis=0)\n",
    "        # the same as: \n",
    "        loss = tf.reduce_max(pred, axis=-1)\n",
    "        # the above line turns pred into one-dimensional: one class name for each sample \n",
    "    grads = tape.gradient(loss, images)\n",
    "    saliency = saliency_to_grayscale(np.abs(grads))\n",
    "\n",
    "    return saliency\n",
    "```\n",
    "Apparently while tensorflow can compute gradients separately for each input, pytorch cannot. This is because tape.gradient() automatically zeros out gradients after each call, but loss.backward() does not. So we need to use a for loop in pytorch's case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e274b7af-46fb-457b-b366-030d0de25dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = vanilla_gradient(model, input_tensor)\n",
    "grads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adf78db-59d8-48f0-b5f3-7e44a41b28e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdb4b06-daf0-4f22-97bd-66a915849995",
   "metadata": {},
   "outputs": [],
   "source": [
    "stepsn.iloc[:,selected_indices].mean(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58674f05-c0a4-474b-8867-d0824dd4746f",
   "metadata": {},
   "source": [
    ":/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c89565-54d9-4c1c-9743-ea5c5bdbb514",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = grads.flatten()  # Make it one-dimensional\n",
    "plt.hist(tensor.detach().numpy(), bins='auto')  # 'auto' automatically determines the number of bins\n",
    "plt.title(\"Histogram of tensor values\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c05533-95dd-4748-a631-04ab87aac3cc",
   "metadata": {},
   "source": [
    "## smoothgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12693c96-d2e6-42ae-a568-22f09e9b6d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothgrad_saliency(model, samples, n_trials = 10, noise_level = 0.1): \n",
    "    gradients = []\n",
    "    for i in range(samples.size(0)): \n",
    "        sample = samples[i] \n",
    "        # add noise \n",
    "        noise = torch.normal(0, noise_level, size = (n_trials,)+sample.shape)\n",
    "        noisy_samples = sample + noise\n",
    "        # the shape here would be n_trials, input_size \n",
    "        gradients.append(torch.mean(vanilla_gradient(model, noisy_samples), axis = 0))\n",
    "    return torch.stack(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e80cd4-2620-42ed-b263-17cad1cf3b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothgrad_saliency(model, samples, n_trials = 10, noise_level = 0.1): \n",
    "    gradients = []\n",
    "    for i in range(samples.size(0)): \n",
    "        sample = samples[i] \n",
    "        # add noise \n",
    "        noise = torch.normal(0, noise_level, size = (n_trials,)+sample.shape)\n",
    "        noisy_samples = sample + noise\n",
    "        print(f'Noisy Samples for input {i}: {noisy_samples[:,1:3]}')  # Debug line\n",
    "\n",
    "        grad = torch.mean(vanilla_gradient(model, noisy_samples), axis = 0)\n",
    "        print(f'Gradient for input {i}: {grad[1:3]}')  # Debug line\n",
    "        gradients.append(grad)\n",
    "    return torch.stack(gradients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44948ac-1385-42a0-bf4a-96a22864aac0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = smoothgrad_saliency(model, input_tensor)\n",
    "out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0dc928-c348-45bf-a73a-32748480b441",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0414f930-c001-4fd6-b25d-d90429ca21ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ce8155-3b41-4e93-8300-31903f5bb6e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93256f9b-ac35-4a06-a996-02faea1ac0d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d4da2a-2944-48e4-8bad-b83fc4a9e472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
